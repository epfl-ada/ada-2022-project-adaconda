{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c0f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35631737",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=6)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698f5c1",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " <div class=\"alert alert-block alert-warning\">\n",
    "For this project we decided to use <a href=\"https://www.dask.org/\" > DASK </a>\n",
    " to carry out a big chunk of the computations </br>\n",
    "Several reasons motivated us to use DASK (instead of PANDAS for example): </br>\n",
    "<p>\n",
    " - DASK is optimized to work with very big files: all the computations are carried out lazily and the whole file is never loaded to memory. The computations are only done when needed and after heavy optimization </p>\n",
    " \n",
    " <p>\n",
    " - DASK makes it very easy to create a local cluster with several machines to make the computationally expensive operations faster (we therefore don't need to deploy our data to something like AWS)\n",
    "  </p>\n",
    "  \n",
    "  <p>\n",
    " - DASK gives us access to a great API which makes it very easy to identify bottlenecks and optimize our computations\n",
    "   </p>\n",
    "    \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329109b5",
   "metadata": {},
   "source": [
    "## Pre-Processing Youtube Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033dd933",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    We take the matadata which is in json format and transform it to parquet to load it in DASK </br>\n",
    "    <b>We decided to use Parquet since it is a columnar data-storage format which also uses compression which allows us to get smaller file and faster queries </b> </br>\n",
    "    We take the matadata which is in json format and transform it to parquet to load it in DASK </br>\n",
    "    In order to open the document we use the chunksize parameter which allows us to load the file in chunks. </br>\n",
    "We then save each one of the chunks as parquet </br>\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"./Data/video_metadata/yt_metadata_en.jsonl.gz\", compression=\"gzip\", lines=True, chunksize=100_000)\n",
    "\n",
    "for i, chunk in enumerate(df):\n",
    "    chunk.to_parquet(f\"./Data/video_metadata/parquet/{i}.parquet\")\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e66f4e",
   "metadata": {},
   "source": [
    "## Comment Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba01ae",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The comment metadata was already available in TSV format. </br>\n",
    "We therefore simply read it using dask and rewrote it in Parquet to make it easier and more efficient to query the data in the future </br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbec4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"./Data/comment_data/youtube_comments.tsv\", sep=\"\\t\")\n",
    "\n",
    "# df.to_parquet('./Data/comment_data/parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712fa4f",
   "metadata": {},
   "source": [
    "## Time Series Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5841ef43",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The time series metadata was already available in TSV format. </br>\n",
    "We therefore simply read it using dask and rewrote it in Parquet to make it easier and more efficient to query the data in the future (same as for comments) </br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"./Data/time_series_data/df_timeseries_en.tsv\", sep=\"\\t\")\n",
    "\n",
    "# df.to_parquet('./Data/time_series_data/parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a01858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
