{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8629633",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> ADA - Milestone 2</h1> \n",
    "\n",
    "**Students:**\n",
    "- ABI FADEL Zad\n",
    "- ADEYE Abiola\n",
    "- BRUNO Etienne\n",
    "- FERCHIOU Sami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d41dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "\n",
    "import numpy as np\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb84091",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn', Mutes warnings when copying a slice from a DataFrame.\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.options.display.max_rows = 20\n",
    "\n",
    "%load_ext ipython_clipboard\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"data/video_metadata/yt_metadata_en.jsonl.gz\", compression=\"gzip\", chunksize=100000, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pd.read_parquet(f\"data/video_metadata/parquet/{i}\") for i in os.listdir(\"data/video_metadata/parquet/\")[:10]]  # only 10 first files to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44dada4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Creation and connection to a database that we will call youniverse. We will then fill it with the following data:  <br>\n",
    "    <ul>\n",
    "        <li> df_channels </li>\n",
    "        <li> df_timeseries_en  </li>\n",
    "        <li> num_comments  </li>\n",
    "        <li> num_comments_authors  </li>\n",
    "        <li> yt_metada_en  </li>\n",
    "        <li> yt_metada_helper  </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caea8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to the main database\n",
    "con = sqlite3.connect(\"databases/youniverse.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7605b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of query\n",
    "cur = con.cursor()\n",
    "res = cur.execute(\"select count(*) from df_channels_en\")\n",
    "res.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c200572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer loop containing subfolders of data\n",
    "folders = os.listdir(\"data\")\n",
    "trash = \".DS_Store\"\n",
    "if trash in folders:\n",
    "    folders.remove(trash)\n",
    "folders.remove(\"ada_west_youniverse\")\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dba16a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p>\n",
    "    The following cell iterate over all data files. It considers only decompressed files, load them and save them into the aforementionned database called \"youniverse.sqlite\". </p>\n",
    "    <p>\n",
    "    We ignore the youtube_comments compressed file as we do not need it in our work and the yt_metadata_en file as we will use Dask to work efficiently with it.\n",
    "    </p>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8bbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer loop for subfolder\n",
    "for folder in folders:\n",
    "    # Remove .DS_Store file of macOS\n",
    "    files = os.listdir(f\"data/{folder}\")\n",
    "    if trash in files:\n",
    "        files.remove(trash)\n",
    "\n",
    "    print(files)\n",
    "    # We only work with tsv or feather file since we have decompressed all gzip files\n",
    "    # Inner loop for data files\n",
    "    for file in files:\n",
    "        name = file.split(\".\")[0]\n",
    "        fmt = file.split(\".\")[1]\n",
    "        file = \"data/\" + folder + \"/\" + file\n",
    "        print(f\"  -> reading {name} file in {fmt} format\")\n",
    "        if len(file.split(\".\")) != 2:  # Ignore compressed files (il still on computer)\n",
    "            print(f\"       >>> ignored\")\n",
    "            continue\n",
    "        if name == \"youtube_comments\":  # Ignore for now the very large comment file\n",
    "            print(f\"       >>> ignored\")\n",
    "            continue\n",
    "        if fmt == \"tsv\":\n",
    "            df = pd.read_csv(file, sep=\"\\t\")\n",
    "        elif fmt == \"feather\":\n",
    "            df = pd.read_feather(file)\n",
    "        elif fmt == \"jsonl\":  # Ignore the yt_metadata_en.jsonl file - we will use parquet files for it\n",
    "            print(f\"       >>> ignored\")\n",
    "            continue\n",
    "        print(f\"       >>> successful\")\n",
    "\n",
    "        # Save dataframe to SQL database\n",
    "        df.to_sql(name, con, if_exists=\"replace\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
