{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924c75be",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> ADA - Milestone 2</h1> \n",
    "\n",
    "**Students:**\n",
    "- ABI FADEL Zad\n",
    "- ADEYE Abiola\n",
    "- BRUNO Etienne\n",
    "- FERCHIOU Sami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94030c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask.distributed import Client\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_workers = multiprocessing.cpu_count()\n",
    "client = Client(n_workers=nb_workers)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd85b74",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " <div class=\"alert alert-block alert-success\">\n",
    "For this project we decided to use <a href=\"https://www.dask.org/\" > DASK </a>\n",
    " to carry out a big chunk of the computations </br>\n",
    "Several reasons motivated us to use DASK (instead of PANDAS for example): </br>\n",
    "<p>\n",
    " - DASK is optimized to work with very big files: all the computations are carried out lazily and the whole file is never loaded to memory. The computations are only done when needed and after heavy optimization </p>\n",
    " \n",
    " <p>\n",
    " - DASK makes it very easy to create a local cluster with several machines to make the computationally expensive operations faster (we therefore don't need to deploy our data to something like AWS)\n",
    "  </p>\n",
    "  \n",
    "  <p>\n",
    " - DASK gives us access to a great API which makes it very easy to identify bottlenecks and optimize our computations\n",
    "   </p>\n",
    "    \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a0796",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " <div class=\"alert alert-block alert-success\">\n",
    "In this jupyter notebbok, we then use dask to load the data by chunks from compressed jsonl or tsv files and save it in parquet which is also a format that dask can easily handle.\n",
    "    \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5196e",
   "metadata": {},
   "source": [
    "## Pre-Processing Youtube Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa1e79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    We take the matadata which is in json format and transform it to parquet to load it in DASK </br>\n",
    "    <b>We decided to use Parquet since it is a columnar data-storage format which also uses compression which allows us to get smaller file and faster queries </b> </br>\n",
    "    We take the matadata which is in json format and transform it to parquet to load it in DASK </br>\n",
    "    In order to open the document we use the chunksize parameter which allows us to load the file in chunks. </br>\n",
    "We then save each one of the chunks as parquet </br>\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07236bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"./data/video_metadata/yt_metadata_en.jsonl.gz\", compression=\"gzip\", lines=True, chunksize=100_000)\n",
    "\n",
    "for i, chunk in enumerate(df):\n",
    "    chunk.to_parquet(f\"./data/video_metadata/parquet/{i}.parquet\")\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b99c0",
   "metadata": {},
   "source": [
    "## Comment Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c0d5f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The comment metadata was already available in TSV format. </br>\n",
    "We therefore simply read it using dask and rewrote it in Parquet to make it easier and more efficient to query the data in the future </br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"./data/comment_data/youtube_comments.tsv\", sep=\"\\t\")\n",
    "\n",
    "df.to_parquet(\"./data/comment_data/parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658103d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e13cd",
   "metadata": {},
   "source": [
    "## Time Series Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4244db8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The time series metadata was already available in TSV format. </br>\n",
    "We therefore simply read it using dask and rewrote it in Parquet to make it easier and more efficient to query the data in the future (same as for comments) </br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"./data/time_series_data/df_timeseries_en.tsv\", sep=\"\\t\")\n",
    "\n",
    "df.to_parquet(\"./data/time_series_data/parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
