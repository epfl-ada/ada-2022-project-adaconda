{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93172a0d",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> ADA - Milestone 2</h1> \n",
    "\n",
    "**Students:**\n",
    "- ABI FADEL Zad\n",
    "- ADEYE Abiola\n",
    "- BRUNO Etienne\n",
    "- FERCHIOU Sami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35631737",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = multiprocessing.cpu_count()\n",
    "client = Client(n_workers=n_workers)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698f5c1",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " <div class=\"alert alert-block alert-success\">\n",
    "For this project we decided to use <a href=\"https://www.dask.org/\" > DASK </a>\n",
    " to carry out a big chunk of the computations </br>\n",
    "Several reasons motivated us to use DASK (instead of PANDAS for example): </br>\n",
    "<p>\n",
    " - DASK is optimized to work with very big files: all the computations are carried out lazily and the whole file is never loaded to memory. The computations are only done when needed and after heavy optimization </p>\n",
    " \n",
    " <p>\n",
    " - DASK makes it very easy to create a local cluster with several machines to make the computationally expensive operations faster (we therefore don't need to deploy our data to something like AWS)\n",
    "  </p>\n",
    "  \n",
    "  <p>\n",
    " - DASK gives us access to a great API which makes it very easy to identify bottlenecks and optimize our computations\n",
    "   </p>\n",
    "    \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d0025",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " <div class=\"alert alert-block alert-success\">\n",
    "In this jupyter notebbok, we then use dask to load the data by chunks from compressed jsonl or tsv files and save it in parquet which is also a format that dask can easily handle.\n",
    "    \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329109b5",
   "metadata": {},
   "source": [
    "## Pre-Processing Youtube Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033dd933",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    We take the matadata which is in json format and transform it to parquet to load it in DASK </br>\n",
    "    <b>We decided to use Parquet since it is a columnar data-storage format which also uses compression which allows us to get smaller file and faster queries </b> </br>\n",
    "    We take the matadata which is in json format and transform it to parquet to load it in DASK </br>\n",
    "    In order to open the document we use the chunksize parameter which allows us to load the file in chunks. </br>\n",
    "We then save each one of the chunks as parquet </br>\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"./Data/video_metadata/yt_metadata_en.jsonl.gz\", compression=\"gzip\", lines=True, chunksize=100_000)\n",
    "\n",
    "for i, chunk in enumerate(df):\n",
    "    chunk.to_parquet(f\"./Data/video_metadata/parquet/{i}.parquet\")\n",
    "    print(i, end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e66f4e",
   "metadata": {},
   "source": [
    "## Comment Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba01ae",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The comment metadata was already available in TSV format. </br>\n",
    "We therefore simply read it using dask and rewrote it in Parquet to make it easier and more efficient to query the data in the future </br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbec4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"./Data/comment_data/youtube_comments.tsv\", sep=\"\\t\")\n",
    "\n",
    "# df.to_parquet('./Data/comment_data/parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712fa4f",
   "metadata": {},
   "source": [
    "## Time Series Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5841ef43",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The time series metadata was already available in TSV format. </br>\n",
    "We therefore simply read it using dask and rewrote it in Parquet to make it easier and more efficient to query the data in the future (same as for comments) </br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"./Data/time_series_data/df_timeseries_en.tsv\", sep=\"\\t\")\n",
    "\n",
    "# df.to_parquet('./Data/time_series_data/parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a01858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
